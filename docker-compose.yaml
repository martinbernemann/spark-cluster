# See howto.md
version: "3.9"

# Shared volume
volumes:
  shared-workspace:
    name: "hadoop-distributed-filesystem"
    driver: local

services:
  # Jupyter Lab
  jupyterlab:
    image: jupyterlab
    container_name: jupyterlab
    environment:
      # For web-ui
      - SPARK_PUBLIC_DNS=localhost
    ports:
      # Jupyter notebooks
      - 8888:8888
      # Spark UI, each SparkContexts gets a new one, starting 4040 
      - 4040-4049:4040-4049
    volumes:
      - shared-workspace:/opt/workspace

  # Spark master
  spark-master:
    image: spark-master
    environment:
      - SPARK_PUBLIC_DNS=localhost
      - SPARK_MASTER_WEBUI_PORT=5050
    ports:
      - 5050:5050
      - 7077:7077 # spark master default port
    volumes:
      - shared-workspace:/opt/workspace

  # Spark workers
  spark-worker-1:
    image: spark-worker
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=512m
      # For web-ui
      - SPARK_PUBLIC_DNS=localhost
      - SPARK_WORKER_WEBUI_PORT=5051
      - SPARK_MASTER_WEBUI_PORT=5050
    ports:
      - 5051:5051
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master

  spark-worker-2:
    image: spark-worker
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=512m
      # For web-ui
      - SPARK_PUBLIC_DNS=localhost
      - SPARK_WORKER_WEBUI_PORT=5052
      - SPARK_MASTER_WEBUI_PORT=5050
    ports:
      - 5052:5052
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master
